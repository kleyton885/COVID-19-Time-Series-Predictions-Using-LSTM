Estudo de algoritmos de Redes Neurais Recorrentes para predição de casos diários pela COVID-19 em Alagoas usando séries temporais com LSTM
> Este tutorial NÃO está tentando construir um modelo que preveja o surto/pandemia de Covid-19 da melhor maneira possível. Este é um exemplo de como você pode usar redes neurais recorrentes em alguns dados de séries temporais do mundo real com o PyTorch. Felizmente, existem modelos muito melhores que preveem o número de casos confirmados diariamente.

Os dados da série temporal capturam uma série de pontos de dados registrados em (geralmente) intervalos regulares. Alguns exemplos comuns incluem a temperatura diária do clima, os preços das ações e o número de vendas que uma empresa faz.

Muitos métodos clássicos (por exemplo, ARIMA) tentam lidar com dados de séries temporais com sucesso variável (para não dizer que são ruins nisso). Nos últimos dois anos, os modelos [Long Short Term Memory Networks (LSTM) tornaram-se um método muito útil ao lidar com esses tipos de dados.

Redes neurais recorrentes (LSTMs são um tipo delas) são muito boas no processamento de sequências de dados. Eles podem "lembrar" padrões nos dados que estão muito distantes no passado (ou futuro). Neste tutorial, você aprenderá como usar LSTMs para prever futuros casos de Coronavírus com base em dados do mundo real.
Segue alguns conceitos sobre as ferramentas de programação utilizadas nesse estudo:

Pytorch é uma biblioteca Python para computação científica. Inclusive é uma das mais utilizadas para pesquisa científica quando se trata de deep learning. O slogan deles é “Da pesquisa para produção”.

A Pytorch é capaz de realizar cálculos utilizando tensores. Tensores podem ser entendidos como vetores n-dimensionais. A vantagem dos tensores da Pytorch é que eles podem ser utilizados tanto em CPUs quanto em GPUs e isso acelera os processos computacionais relacionados ao deep learning.

ref: https://www.alura.com.br/artigos/primeiros-passos-com-pytorch

---

NumPy é uma biblioteca Python usada para trabalhar com arrays. Também possui funções para trabalhar no domínio da álgebra linear, transformada de Fourier e matrizes.

Em Python, temos listas que servem ao propósito de arrays, mas são lentas para processar. O NumPy visa fornecer um objeto de matriz até 50 vezes mais rápido que as listas tradicionais do Python.

ref: https://www.w3schools.com/python/numpy/numpy_intro.asp

---

Pandas é um pacote Python que fornece estruturas de dados rápidas, flexíveis e expressivas projetadas para tornar o trabalho com dados “relacionais” ou “rotulados” fácil e intuitivo. Ele visa ser o bloco de construção fundamental de alto nível para fazer análises práticas de dados do mundo real em Python. Além disso, tem o objetivo mais amplo de se tornar a ferramenta de análise/manipulação de dados de código aberto mais poderosa e flexível disponível em qualquer idioma.

ref: https://pandas.pydata.org/docs/getting_started/overview.html

---

Seaborn é uma biblioteca de visualização de dados Python baseada em matplotlib. Ele fornece uma interface de alto nível para desenhar gráficos estatísticos atraentes e informativos.

ref: https://seaborn.pydata.org/

---

O Pylab é um módulo da linguagem Python que permite gerar gráficos de duas dimensões de excelente qualidade, permitindo edição interativa, animações, inúmeros tipos de gráficos diferentes, anotações em sintaxe Latex e salvamento das imagens geradas em diversos formatos diferentes. A sintaxe de criação e manipulação das imagens será familiar para quem já trabalhou com o software comercial Matlab, mas provendo muito mais capacidades, além de uma interface baseada em objetos, para quem conhece a técnica.

O Pylab permite trabalhar com diversos tipos de gráficos diferentes, entre eles: gráficos de funções, múltiplos gráficos, histogramas, funções discretas, torta, barra, etc. Fornece funções para a customização dos gráficos, podendo trabalhar com diversas fontes diferentes, cores, tamanhos de página, e muito mais. Além disso, existem funções para a manipulação e análise de imagens e sinais. Veja abaixo uma imagem gerada pelo Pylab:

ref: http://pyscience-brasil.wikidot.com/module:pylab

---

matplotlib - é uma biblioteca de plotagem orientada a objetos.
ref: https://matplotlib.org/stable/api/matplotlib_configuration_api.html


---

Scikit-learn - é uma biblioteca de aprendizado de máquina de código aberto que suporta aprendizado supervisionado e não supervisionado. Ele também fornece várias ferramentas para ajuste de modelo, pré-processamento de dados, seleção de modelo, avaliação de modelo e muitos outros utilitários.

ref: https://scikit-learn.org/stable/getting_started.html
import torch

import numpy as np
import pandas as pd
import seaborn as sns
import pylab
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from pandas.plotting import register_matplotlib_converters
from torch import nn, optim

%matplotlib inline
%config InlineBackend.figure_format='retina'

sns.set(style='whitegrid', palette='muted', font_scale=1.2)

HAPPY_COLORS_PALETTE = ["#01BEFE", "#FFDD00", "#FF7D00", "#FF006D", "#93D30C", "#8F00FF"]

sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))

pylab.rcParams['figure.figsize'] = 14, 10
register_matplotlib_converters()

RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
torch.manual_seed(RANDOM_SEED)
## Daily Cases Dataset
A primeira etapa é realizar a aquisição dos dados. Os dados ultizados nesse estudo são fornecidos pelo Instituto de Métricas e Avaliação em Saúde, mundialmente conhecido como IHME (Institute for Health Metrics and Evaluation) e contêm o número de casos diários de COVID-19, além de muitas outras informações relacionadas a essa e outras doenças.
!wget https://ihmecovid19storage.blob.core.windows.net/archive/2022-11-18/data_download_file_reference_2022.csv
# Para compor nossa série temporal, é preciso um csv onde a primeira coluna seja o valor da data, e a segunda coluna seja o número de casos, logo faremos a preparação dos dados.

df = pd.read_csv('data_download_file_reference_2022.csv')
df = df.loc[(df['location_name'] == "Alagoas") & (df['daily_cases'].notnull())]
df = df[["date", "daily_cases"]].tail(60)
df = pd.Series([int(a) for a in df["daily_cases"]], index = df["date"])
daily_cases = df.squeeze()
daily_cases.index = pd.to_datetime(daily_cases.index)
daily_cases.head()
plt.plot(daily_cases)
plt.xticks(rotation=60, ha='right')
plt.title("Casos Diários");
daily_cases.shape
test_data_size = 20

train_data = daily_cases[:-test_data_size]
test_data = daily_cases[-test_data_size:]

train_data.shape
Temos que escalar os dados (os valores estarão entre 0 e 1) se quisermos aumentar a velocidade de treinamento e desempenho do modelo. Usaremos o MinMaxScaler do scikit-learn:
scaler = MinMaxScaler()

scaler = scaler.fit(np.expand_dims(train_data, axis=1))

train_data = scaler.transform(np.expand_dims(train_data, axis=1))

test_data = scaler.transform(np.expand_dims(test_data, axis=1))
Atualmente, temos uma grande sequência de casos diários. Vamos convertê-lo em menores:
def create_sequences(data, seq_length):
    xs = []
    ys = []

    for i in range(len(data)-seq_length-1):
        x = data[i:(i+seq_length)]
        y = data[i+seq_length]
        xs.append(x)
        ys.append(y)

    return np.array(xs), np.array(ys)
seq_length = 5
X_train, y_train = create_sequences(train_data, seq_length)
X_test, y_test = create_sequences(test_data, seq_length)

X_train = torch.from_numpy(X_train).float()
y_train = torch.from_numpy(y_train).float()

X_test = torch.from_numpy(X_test).float()
y_test = torch.from_numpy(y_test).float()
Cada exemplo de treinamento contém uma sequência de 5 pontos de dados do histórico e um rótulo para o valor real que nosso modelo precisa prever. Vamos mergulhar em:
X_train.shape
X_train[:2]
y_train.shape
y_train[:2]
train_data[:10]
## Contruindo o Modelo

Vamos encapsular a complexidade do nosso modelo em uma classe que se estende de `torch.nn.Module`:
class CoronaVirusPredictor(nn.Module):

  def __init__(self, n_features, n_hidden, seq_len, n_layers=2):
    super(CoronaVirusPredictor, self).__init__()

    self.n_hidden = n_hidden
    self.seq_len = seq_len
    self.n_layers = n_layers

    self.lstm = nn.LSTM(
      input_size=n_features,
      hidden_size=n_hidden,
      num_layers=n_layers,
      dropout=0.5
    )

    self.linear = nn.Linear(in_features=n_hidden, out_features=1)

  def reset_hidden_state(self):
    self.hidden = (
        torch.zeros(self.n_layers, self.seq_len, self.n_hidden),
        torch.zeros(self.n_layers, self.seq_len, self.n_hidden)
    )

  def forward(self, sequences):
    lstm_out, self.hidden = self.lstm(
      sequences.view(len(sequences), self.seq_len, -1),
      self.hidden
    )
    last_time_step = \
      lstm_out.view(self.seq_len, len(sequences), self.n_hidden)[-1]
    y_pred = self.linear(last_time_step)
    return y_pred
Our `CoronaVirusPredictor` contains 3 methods:
- constructor - initialize all helper data and create the layers
- `reset_hidden_state` - we'll use a stateless LSTM, so we need to reset the state after each example
- `forward` - get the sequences, pass all of them through the LSTM layer, at once. We take the output of the last time step and pass it through our linear layer to get the prediction.
## Treinamento

Vamos construir uma função auxiliar para o treinamento do nosso modelo (vamos reutilizá-la mais tarde):
def train_model(
  model, 
  train_data, 
  train_labels, 
  test_data=None, 
  test_labels=None
):
  loss_fn = torch.nn.MSELoss(reduction='sum')

  optimiser = torch.optim.Adam(model.parameters(), lr=1e-3)
  num_epochs = 60

  train_hist = np.zeros(num_epochs)
  test_hist = np.zeros(num_epochs)

  for t in range(num_epochs):
    model.reset_hidden_state()

    y_pred = model(X_train)

    loss = loss_fn(y_pred.float(), y_train)

    if test_data is not None:
      with torch.no_grad():
        y_test_pred = model(X_test)
        test_loss = loss_fn(y_test_pred.float(), y_test)
      test_hist[t] = test_loss.item()

      if t % 10 == 0:  
        print(f'Epoch {t} train loss: {loss.item()} test loss: {test_loss.item()}')
    elif t % 10 == 0:
      print(f'Epoch {t} train loss: {loss.item()}')

    train_hist[t] = loss.item()
    
    optimiser.zero_grad()

    loss.backward()

    optimiser.step()
  
  return model.eval(), train_hist, test_hist
Observe que o estado oculto é redefinido no início de cada época. Não usamos lotes de dados, nosso modelo vê todos os exemplos de uma só vez. Usaremos o erro quadrático médio para medir nosso erro de treinamento e teste. Nós vamos gravar os dois.

Vamos criar uma instância do nosso modelo e treiná-la:
model = CoronaVirusPredictor(
  n_features=1, 
  n_hidden=512, 
  seq_len=seq_length, 
  n_layers=2
)
model, train_hist, test_hist = train_model(
  model, 
  X_train, 
  y_train, 
  X_test, 
  y_test
)
Vamos dar uma olhada no train and test loss:
plt.plot(train_hist, label="Training loss")
plt.plot(test_hist, label="Validation loss")
plt.ylim((0, 10))
plt.legend();
O desempenho do nosso modelo não melhora após 15 épocas ou mais. Lembre-se de que temos poucos dados. Talvez não devêssemos confiar tanto em nosso modelo?
## Prevendo Casos Diários

Nosso modelo pode (devido à forma como o treinamos) prever apenas um único dia no futuro. Vamos empregar uma estratégia simples para superar essa limitação. Use valores previstos como entrada para prever os próximos dias:
with torch.no_grad():
  test_seq = X_test[:1]
  preds = []
  for _ in range(len(X_test)):
    y_test_pred = model(test_seq)
    pred = torch.flatten(y_test_pred).item()
    preds.append(pred)
    new_seq = test_seq.numpy().flatten()
    new_seq = np.append(new_seq, [pred])
    new_seq = new_seq[1:]
    test_seq = torch.as_tensor(new_seq).view(1, seq_length, 1).float()
Temos que inverter a escala dos dados de teste e as previsões do modelo:
true_cases = scaler.inverse_transform(
    np.expand_dims(y_test.flatten().numpy(), axis=0)
).flatten()

predicted_cases = scaler.inverse_transform(
  np.expand_dims(preds, axis=0)
).flatten()
Vejamos os resultados:
plt.plot(
  daily_cases.index[:len(train_data)], 
  scaler.inverse_transform(train_data).flatten(),
  label='Histórico de Casos Diários'
)

plt.plot(
  daily_cases.index[len(train_data):len(train_data) + len(true_cases)], 
  true_cases,
  label='Casos diários Reais'
)

plt.plot(
  daily_cases.index[len(train_data):len(train_data) + len(true_cases)], 
  predicted_cases, 
  label='Casos Diários Previstos'
)

plt.xticks(rotation=60, ha='right')

plt.legend();
Como esperado, nosso modelo não funciona muito bem. Dito isso, as previsões parecem estar no caminho certo (provavelmente devido ao uso do último ponto de dados como um forte preditor para o próximo).
## Usando todos os dados para treinamento

Agora, usaremos todos os dados disponíveis para treinar o mesmo modelo:
scaler = MinMaxScaler()

scaler = scaler.fit(np.expand_dims(daily_cases, axis=1))

all_data = scaler.transform(np.expand_dims(daily_cases, axis=1))

all_data.shape
As etapas de pré-processamento e treinamento são as mesmas:
X_all, y_all = create_sequences(all_data, seq_length)

X_all = torch.from_numpy(X_all).float()
y_all = torch.from_numpy(y_all).float()

model = CoronaVirusPredictor(
  n_features=1, 
  n_hidden=512, 
  seq_len=seq_length, 
  n_layers=2
)
model, train_hist, _ = train_model(model, X_all, y_all)
Prevendo Casos Futuros

Usaremos nosso modelo "totalmente treinado" para prever os casos confirmados por 12 dias no futuro:
DAYS_TO_PREDICT = 12

with torch.no_grad():
  test_seq = X_all[:1]
  preds = []
  for _ in range(DAYS_TO_PREDICT):
    y_test_pred = model(test_seq)
    pred = torch.flatten(y_test_pred).item()
    preds.append(pred)
    new_seq = test_seq.numpy().flatten()
    new_seq = np.append(new_seq, [pred])
    new_seq = new_seq[1:]
    test_seq = torch.as_tensor(new_seq).view(1, seq_length, 1).float()
Como antes, inverteremos a transformação do scaler:
predicted_cases = scaler.inverse_transform(
  np.expand_dims(preds, axis=0)
).flatten()
Para criar um gráfico legal com os casos históricos e previstos, precisamos estender o índice de data do nosso quadro de dados:
daily_cases.index[-1]
predicted_index = pd.date_range(
  start=daily_cases.index[-1],
  periods=DAYS_TO_PREDICT + 1,
  closed='right'
)

predicted_cases = pd.Series(
  data=predicted_cases,
  index=predicted_index
)

plt.plot(predicted_cases, label='Predicted Daily Cases')
plt.legend();
Agora podemos usar todos os dados para plotar os resultados:
plt.plot(daily_cases, label='Histórico de Casos Diários')
plt.plot(predicted_cases, label='Casos Diários Previstos')
plt.legend();
Nosso modelo pensa que as coisas vão se estabilizar. Observe que quanto mais você avança no futuro, mais não deve confiar nas previsões do modelo.
## Conclusão

Bem feito! Você aprendeu a usar o PyTorch para criar uma rede neural recorrente que funciona com dados de séries temporais. O desempenho do modelo não é tão bom, mas isso é esperado, dada a pequena quantidade de dados.

- [Run the complete notebook in your browser (Google Colab)](https://colab.research.google.com/drive/1nQYJq1f7f4R0yeZOzQ9rBKgk00AfLoS0)
- [Read the Getting Things Done with Pytorch book](https://github.com/curiousily/Getting-Things-Done-with-Pytorch)

O problema de prever casos diários de Covid-19 é difícil. Estamos em meio a um surto e há mais a ser feito. Espero que tudo volte ao normal depois de algum tempo.
## Referências

- [Time Series Forecasting with LSTMs for Daily Coronavirus Cases using PyTorch in Python](https://curiousily.com/posts/time-series-forecasting-with-lstm-for-daily-coronavirus-cases/)

- [IHME - COVID-19 estimate downloads](https://www.healthdata.org/covid/data-downloads)

- [Sequence Models PyTorch Tutorial](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html)
- [LSTM for time series prediction](https://towardsdatascience.com/lstm-for-time-series-prediction-de8aeb26f2ca)
- [Time Series Prediction using LSTM with PyTorch in Python](https://stackabuse.com/time-series-prediction-using-lstm-with-pytorch-in-python/)
- [Stateful LSTM in Keras](https://philipperemy.github.io/keras-stateful-lstm/)
- [LSTMs for Time Series in PyTorch](https://www.jessicayung.com/lstms-for-time-series-in-pytorch/)
- [Novel Coronavirus (COVID-19) Cases, provided by JHU CSSE](https://github.com/CSSEGISandData/COVID-19)
- [covid-19-analysis](https://github.com/AaronWard/covid-19-analysis)
- [How does Coronavirus compare to Ebola, SARS, etc?](https://www.youtube.com/watch?v=6dDD2tHWWnU)
- [Worldometer COVID-19 Coronavirus Outbreak](https://www.worldometers.info/coronavirus/)
- [How contagious is the Wuhan Coronavirus? (Ro)](https://www.worldometers.info/coronavirus/#repro)
- [Systemic Risk of Pandemic via Novel Pathogens - Coronavirus: A Note](https://www.academia.edu/41743064/Systemic_Risk_of_Pandemic_via_Novel_Pathogens_-_Coronavirus_A_Note)
- [Statistical Consequences of Fat Tails: Real World Preasymptotics, Epistemology, and Applications](https://www.researchers.one/article/2020-01-21)
